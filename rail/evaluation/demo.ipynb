{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: RAIL Evaluation \n",
    "\n",
    "Contact: _Julia Gschwend_ ([julia@linea.gov.br](mailto:julia@linea.gov.br)), _Sam Schmidt, Alex Malz, Eric Charles_\n",
    "\n",
    "\n",
    "The purpose of this notebook is to demonstrate the use of the metrics scripts to be used on the photo-$z$ PDF catalogs produced by the PZ working group. The first implementation of the _evaluation_ module is based on the refactoring of the algorithms used in [Schmidt et al. 2020](https://arxiv.org/pdf/2001.03621.pdf), available on Github repository [PZDC1paper](https://github.com/LSSTDESC/PZDC1paper). \n",
    "\n",
    "To run this code, you must install qp and have the notebook in the same directory as metrics.py. You must also install some run-of-the-mill Python packages: matplotlib, numpy, scipy, and skgof.\n",
    "\n",
    "### Contents\n",
    "\n",
    "* [Sample](#sample)\n",
    " - [Run FZBoost](#fzboost)\n",
    " - [Traditional validation plots](#old_valid)  \n",
    "* [Metrics](#cdf_metrics)\n",
    " - [PIT](#pit) \n",
    " - [QQ plot](#qq) \n",
    " - [CDE loss](#cde_loss)  \n",
    "* [Summary statistics](#summary_stats)\n",
    " - [KS](#ks) \n",
    " - [CvM](#cvm) \n",
    " - [AD](#ad) \n",
    "* [Summary](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "from sample import Sample\n",
    "from metrics import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sample\"></a>\n",
    "\n",
    "# Sample  \n",
    "\n",
    "\n",
    "To compute the photo-z metrics of a given test sample, it is necessary to read the output of a photo-z code containing galaxies' photo-z PDFs. Let's use the toy data available in `tests/data/` (**test_dc2_training_9816.hdf5** and **test_dc2_validation_9816.hdf5**) and the configuration file available in `examples/configs/FZBoost.yaml` to generate a small samples of photo-z PDFs using the **FZBoost** algorithm available on RAIL's _estimation_ module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"fzboost\"></a>\n",
    "### Run FZBoost\n",
    "\n",
    "Go to dir  `<your_path>/RAIL/examples/` and run the command:\n",
    "\n",
    "`python main.py configs/FZBoost.yaml`\n",
    "\n",
    "The photo-z output files (inputs for this notebook) will be writen at: \n",
    "\n",
    "`<your_path>/RAIL/examples/results/FZBoost/test_FZBoost.hdf5`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>The new RAIL's version will produce output of the codes as qp files rather than the old format hdf5 files (Sam's message on Slack about RAIL's issue#33). TO DO: update the read() function of class Sample </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '/Users/julia/github/RAIL' # replace it by your path to RAIL's parent dir\n",
    "pdfs_file = my_path + '/examples/results/FZBoost/test_FZBoost.hdf5'\n",
    "ztrue_file = my_path + '/tests/data/test_dc2_validation_9816.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Sample object containing both the PDFs and true redshifts for each photo-z code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Sample(pdfs_file, ztrue_file, code=\"FZBoost\", name=\"toy data\")\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDFs of 5 galaxies for illustration. The function `plot_pdfs` calls a _qp_ built-in plot function and returns the color codes of galaxies whose indexes are include in the list `gals`. The galaxies in the example were chosen arbitrarily to cover the sample's redshift space. The dashed lines represent their respective true redshifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gals = np.random.choice(len(ztrue), 5)\n",
    "gals = [540, 2256, 12175, 17802, 19502]\n",
    "colors = sample.plot_pdfs(gals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"old_valid\"></a>\n",
    "### Validation plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional validation plots. The point colors (optional) follow the same color code as the PDFs above. \n",
    "\n",
    "<font color='red'>TO DO: update the plots below to look like Figure 4 from CHIPPR's paper ([Malz & Hogg 2020](https://arxiv.org/pdf/2007.12178.pdf)). </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.plot_old_valid(gals=gals, colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"cdf_metrics\"></a>\n",
    "# CDF-based metrics\n",
    "\n",
    "The folowing metrics are computed based on the photo-z PDFs. Let's create a Metrics object to access the basic metrics (e.g., PIT outlier rate, defined below) and basic plots. It is the parent class of other particular metrics. \n",
    "\n",
    "Instantiating a Metrics object can take a bit long, depending on the sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics = Metrics(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"pit\"></a>\n",
    "\n",
    "## PIT\n",
    "\n",
    "The first metric we calculate is the Probability Integral Transform (PIT), which is the Cumulative Distribution Function (CDF) \n",
    "\n",
    "$$ \\mathrm{CDF}(f, q)\\ =\\ \\int_{-\\infty}^{q}\\ f(z)\\ dz $$\n",
    "\n",
    "evaluated at the galaxy's true redshift for every galaxy $i$ in the catalog.\n",
    "\n",
    "$$ \\mathrm{PIT}(p_{i}(z);\\ z_{i})\\ =\\ \\int_{-\\infty}^{z^{true}_{i}}\\ p_{i}(z)\\ dz $$ \n",
    "\n",
    "For instance, the PIT values for the 5 PDFs shown above are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.pit[gals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PIT outlier rate\n",
    "\n",
    "The PIT outlier rate is a global metric defined as the fraction of galaxies in the sample with extreme PIT values (PIT $<10^{-4}$ or PIT $>0.9999$). The lower and upper limits for considering a PIT as outlier are optional parameters set at the Metrics instantiation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PIT outlier rate of this sample: {metrics.pit_out_rate:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIT-QQ plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of PIT values is a useful tool for a qualitative assessment of PDFs quality. It shows whether the PDFs are:\n",
    "* biased (tilted PIT histogram)\n",
    "* under-dispersed (excess counts close to the boudaries 0 and 1)\n",
    "* over-dispersed (lack of counts close the boudaries 0 and 1)\n",
    "* well-calibrated (flat histogram)\n",
    "\n",
    "Following the standards in DC1 paper, the PIT histogram is accompanied by the quantile-quantile (QQ), which can be used to compare qualitatively the PIT distribution obtained with the PDFs agaist the ideal case (uniform distribution). The closer the QQ plot is to the diagonal, the better is the PDFs calibration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_pit_qq() #savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the function `plot_pit_qq` displays both PIT histogram and the QQ plots together. The title and label are retrieved from sample's attributed for sample name and the photo-z (if not informed as optional parameters). It is also possible to select one plot at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_pit_qq(show_pit=False, show_pit_out_rate=False, title=\"QQ only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible ti set the number of bins in the PIT histigram. By default, it uses the same number of quantiles, which is an attribute of the metrics object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.n_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_pit_qq(show_qq=False, title=\"PIT only\", bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black horizontal line represents the ideal case where the PIT histogram would behave as a uniform distribution U(0,1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a different number of quantiles. By default, a Metrics object is instantiated with parameter `n_quant` = 100 percentiles. To redefine the number of quantiles, let's create a new Metrics object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_20 = Metrics(sample, n_quant=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_20.plot_pit_qq(title=\"N$_{quantiles}$=20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"cde_loss\"></a>\n",
    "## CDE Loss\n",
    "\n",
    "Next we can calculate the Conditional Density Estimate (CDE) loss described in [Izbicki & Lee, 2017 (arXiv:1704.08095)](https://arxiv.org/abs/1704.08095). \n",
    "\n",
    "$$ \\int \\int ((p(z \\mid x) - \\hat{p}(z \\mid x))^{2} dz dP(x) $$\n",
    "\n",
    "which extends L2 density estimation loss to conditional density estimation.  We can estimate this quantity (up to an unknown additive constant which depends on the true conditional densities) from data as\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^{n} \\int \\hat{p}^{2}(z \\mid x_{i}) dz - \\frac{2}{n} \\sum_{i=1}^{n} \\hat{p}(z_{i} \\mid x_{i}) $$\n",
    "\n",
    "<font color='red'> TO DO: update equations to match the paper </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CDE loss of this sample: {metrics.cde_loss:.2f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"summary_stats\"></a>\n",
    "# Summary statistics of CDF-based metrics\n",
    "\n",
    "To evaluate globally the quality of PDFs estimates, we compute a number of statistical tests to compare the empirical distributions of PIT values with the theoretically perfect  distribution, U(0,1).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual metrics are implemented as independent classes receiving the PIT array as input. They can be accessed separately or incorporated to the generic metrics object to compose the sumary of all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ks\"></a>\n",
    "### Kolmogorov-Smirnov  \n",
    "\n",
    "We start with the Kolmogorov-Smirnov (KS) statistic test, which is the maximum difference between the empirical and the expected distributions:\n",
    "\n",
    "$$\n",
    "\\mathrm{KS} \\equiv \\max_{z} \\Big( \\left| \\ \\mathrm{CDF} \\small[ \\hat{f}, z \\small] - \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\  \\right| \\Big)\n",
    "$$\n",
    "\n",
    "so the smaller value of KS the closer the PIT distribution is to be uniform. The KS metric inherits from scipy.stats.kstest method, returning the KS maximum distance and the p-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = KS(metrics._pit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"cvm\"></a>\n",
    "### Cramer-von Mises\n",
    "\n",
    "Similarly, we calculate the Cramer-von Mises (CvM) test, a variant of the KS statistic defined as the mean-square difference between the CDFs of an empirical PDF and the true PDFs:\n",
    "\n",
    "$$ \\mathrm{CvM}^2 \\equiv \\int_{-\\infty}^{\\infty} \\Big( \\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\Big)^{2} \\mathrm{dCDF}(\\tilde{f}, z) $$ \n",
    "\n",
    "\n",
    "on the distribution of PIT values, which should be uniform if the PDFs are perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm = CvM(metrics._pit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm.stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ad\"></a>\n",
    "### Anderson-Darling \n",
    "\n",
    "Another variation of the KS statistic is the Anderson-Darling (AD) test, a weighted mean-squared difference featuring enhanced sensitivity to discrepancies in the tails of the distribution. \n",
    "\n",
    "$$ \\mathrm{AD}^2 \\equiv N_{tot} \\int_{-\\infty}^{\\infty} \\frac{\\big( \\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\big)^{2}}{\\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\big( 1 \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\big)}\\mathrm{dCDF}(\\tilde{f}, z) $$ \n",
    "\n",
    "where catastrophic outliers (default is PIT<0.01 or PIT>0.99) are removed for preserving numerical instability. Note that these values are not the same used to define the PIT outlier rate above (0.0001, and 0.9999). \n",
    "\n",
    "The AD metric also inherits from scipy.stats library, returning three quantities: the AD stat value, a list of significance levels , and a list of critical values, and regarding those significance levels.\n",
    "\n",
    "<font color='red'> QUESTION: scipy.stats.anderson method supports the following distributions as reference: {‘norm’, ‘expon’, ‘logistic’, ‘gumbel’, ‘gumbel_l’, ‘gumbel_r’, ‘extreme1’}. Apparently there is no way to compare the PIT distribution with a uniform distribution using this method. I'm using the default option ('norm'), but I think it's wrong.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = AD(metrics._pit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.significance_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.critical_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> TO DO: add sentence with interpretation to the numbers above</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"summary\"></a>\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_table = metrics.markdown_summary() \n",
    "Markdown(metrics_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> TO DO: Metrics plot (Figure 3 from DC1 Paper: [Schmidt et al. 2020](https://arxiv.org/pdf/2001.03621.pdf) )</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
