{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAIL Evaluation - Check results against DC1 paper\n",
    "\n",
    "Contact: _Julia Gschwend_ ([julia@linea.gov.br](mailto:julia@linea.gov.br)), _Sam Schmidt, Alex Malz, Eric Charles_\n",
    "\n",
    "The purpose of this notebook is to validate the new implementation of the DC1 metrics, previously available on Github repository [PZDC1paper](https://github.com/LSSTDESC/PZDC1paper), now refactored to be part of RAIL Evaluation module. The metrics here were implemented in object-oriented Python 3, inheriting features from _qp_. In this notebook we use the same input dataset used in DC1 PZ paper ([Schmidt et al. 2020](https://arxiv.org/pdf/2001.03621.pdf)), copied from cori (/global/cfs/cdirs/lsst/groups/PZ/PhotoZDC1/photoz_results/TESTDC1FLEXZ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from sample import Sample\n",
    "from metrics import *\n",
    "import os\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sample\"></a>\n",
    "\n",
    "## Sample  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = \"/Users/julia/TESTDC1FLEXZ\"\n",
    "\n",
    "pdfs_file =  os.path.join(my_path, \"Mar5Flexzgold_pz.out\")\n",
    "ztrue_file =  os.path.join(my_path, \"Mar5Flexzgold_idszmag.out\")\n",
    "\n",
    "#pdfs_file =  os.path.join(my_path, \"1pct_Mar5Flexzgold_pz.out\")\n",
    "#ztrue_file =  os.path.join(my_path, \"1pct_Mar5Flexzgold_idszmag.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Sample(pdfs_file, ztrue_file, code=\"FlexZBoost\", name=\"DC1 paper data\")\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics = Metrics(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics below are based on the PIT and the CDF(PIT), both computed via qp.Ensemble object method. The PIT array is computed as the qp.Ensemble CDF function for an object containing the photo-z PDFs, evaluated at the true $z$ for each galaxy. \n",
    "\n",
    "```python\n",
    "        n = len(self._sample)\n",
    "        self._pit = np.array([self._sample._pdfs[i].cdf(self._sample._ztrue[i])[0][0]\n",
    "                              for i in range(n)])\n",
    "        Qtheory = np.linspace(0., 1., self.n_quant)\n",
    "        Qdata = np.quantile(self._pit, Qtheory)\n",
    "        self._qq_vectors = (Qtheory, Qdata)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The PIT distribution is implemented as the normalized histogram of PIT values and the uniform U(0,1) is implemented as a mock normalized distribution with the same number of bins of PIT distribution, where all values are equal to $\\frac{1.}{N_{quant}}$.     \n",
    "\n",
    "```python         \n",
    "        self._yscale_uniform = 1. / float(n_quant) # Uniform distribution amplitude\n",
    "        self._xvals = Qtheory\n",
    "        self._pit_dist, self._pit_bins_edges = np.histogram(self._pit, bins=n_quant, density=True) # Distribution of PIT values as it was a PDF\n",
    "        self._uniform_dist = np.ones_like(self._pit_dist) * self._yscale_uniform\n",
    " ```       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a new qp.Ensemble object is instantiated for each distribution, PITs and U(0,1), to use the CDF functionallity (an ensemble with only 1 PDF each).\n",
    "\n",
    " ```python\n",
    "        \n",
    "        self._pit_ensamble = qp.Ensemble(qp.hist, data=dict(bins=self._pit_bins_edges,\n",
    "                                                            pdfs=np.array([self._pit_dist])))\n",
    "        self._uniform_ensamble = qp.Ensemble(qp.interp, data=dict(xvals=self._xvals,\n",
    "                                                                  yvals=np.array([self._uniform_dist])))\n",
    "        self._pit_cdf = self._pit_ensamble.cdf(self._xvals)[0]\n",
    "        self._uniform_cdf = self._uniform_ensamble.cdf(self._xvals)[0]\n",
    "```\n",
    "\n",
    "\n",
    "Where $\\mathrm{CDF} \\small[ \\hat{f}, z \\small]$ represents `self._pit_cdf`  and $\\mathrm{CDF} \\small[ \\tilde{f}, z \\small]$ represents  `self._uniform_cdf` in the equations below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PIT-QQ plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_pit_qq() #savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> Comment: it doesn't look like the FlexZBoost's panel in Figure 2. There is something inconsistent. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DC1 results are stored in Metrics class object as a dictionary, inheriting from an independent class DC1 which exists only to provide the reference values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.dc1['PIT out rate']['FlexZBoost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is necessary to consult the names (keys) of metrics or codes available, the user can call the class DC1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc1 = DC1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc1.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc1.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc1.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary table with all metrics containing DC1 paper results for comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.markdown_table(show_dc1=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first attempt, the results do not match. The PIT outliers rate and CDE loss are close to the reference values. The KS, CvM, and AD tests still need to be fixed. Let's investigate these numbers by comparing the results with what we would get if using the scipy built-in statistical tests (implemented as alternative methods for each metric). \n",
    "\n",
    "### Kolmogorov-Smirnov  \n",
    "\n",
    "$$\n",
    "\\mathrm{KS} \\equiv \\max_{PIT} \\Big( \\left| \\ \\mathrm{CDF} \\small[ \\hat{f}, z \\small] - \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\  \\right| \\Big)\n",
    "$$\n",
    "\n",
    "```python\n",
    "    def __init__(self, metrics, scipy=False):\n",
    "        self._metrics = metrics\n",
    "        if scipy:\n",
    "            self._stat, self._pvalue = stats.kstest(metrics._pit, \"uniform\")\n",
    "        else:\n",
    "            self._stat, self._pvalue = np.max(np.abs(metrics._pit_cdf - metrics._uniform_cdf)), None # p=value TBD\n",
    "        # update Metrics object\n",
    "        metrics._ks_stat = self._stat\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = KS(metrics).stat\n",
    "ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_sp = KS(metrics, scipy=True).stat\n",
    "ks_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Komolgorof-Smirnov test, the values with and without scipy.stats.ks_test function are pretty close. The small diference may be explained by minor differences in the two methods, e.g. binning definition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cramer-von Mises\n",
    "\n",
    "\n",
    "$$ \\mathrm{CvM}^2 \\equiv \\int_{-\\infty}^{\\infty} \\Big( \\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\Big)^{2} \\mathrm{dCDF}(\\tilde{f}, z) $$ \n",
    "\n",
    "```python\n",
    "\n",
    "    def __init__(self, metrics, scipy=False):\n",
    "        if scipy:\n",
    "            cvm_result = stats.cramervonmises(metrics._pit_dist, \"uniform\")\n",
    "            self._stat, self._pvalue = cvm_result.statistic, cvm_result.pvalue\n",
    "        else:\n",
    "            self._stat, self._pvalue = np.sqrt(np.trapz((metrics._pit_cdf - metrics._uniform_cdf)**2, metrics._uniform_cdf)), None # p-value TBD\n",
    "        # update Metrics object\n",
    "        metrics._cvm_stat = self._stat\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm = CvM(metrics).stat\n",
    "cvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm_sp = CvM(metrics, scipy=True).stat\n",
    "cvm_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Cramer-von Mises test, when using the scipy.stats.cramervonmises method we get closer to the reference value, but not close enough yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anderson-Darling \n",
    "\n",
    "$$ \\mathrm{AD}^2 \\equiv N_{tot} \\int_{-\\infty}^{\\infty} \\frac{\\big( \\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\big)^{2}}{\\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\big( 1 \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\big)}\\mathrm{dCDF}(\\tilde{f}, z) $$ \n",
    "\n",
    "```python\n",
    "    def __init__(self, metrics, ad_pit_min=0.0, ad_pit_max=1.0, scipy=False):\n",
    "\n",
    "        mask_pit = (metrics._pit >= ad_pit_min) & (metrics._pit  <= ad_pit_max)\n",
    "        if (ad_pit_min != 0.0) or (ad_pit_max != 1.0):\n",
    "            n_out = len(metrics._pit) - len(metrics._pit[mask_pit])\n",
    "            perc_out = (float(n_out)/float(len(metrics._pit)))*100.\n",
    "            print(f\"{n_out} outliers (PIT<{ad_pit_min} or PIT>{ad_pit_max}) removed from the calculation ({perc_out:.1f}%)\")\n",
    "        if scipy:\n",
    "            #self._stat, self._critical_values, self._significance_levels = stats.anderson(metrics._pit[mask_pit])\n",
    "            self._stat, self._critical_values, self._significance_levels = None, None, None\n",
    "            print(\"Comparison to uniform distribution is not available in scipy.stats.anderson method.\")\n",
    "        else:\n",
    "            ad_xvals = np.linspace(ad_pit_min, ad_pit_max, metrics._n_quant)\n",
    "            ad_yscale_uniform = (ad_pit_max-ad_pit_min)/float(metrics._n_quant)\n",
    "            ad_pit_dist, ad_pit_bins_edges = np.histogram(metrics._pit[mask_pit], bins=metrics._n_quant, density=True)\n",
    "            ad_uniform_dist = np.ones_like(ad_pit_dist) * ad_yscale_uniform\n",
    "            # Redo CDFs to consider outliers mask\n",
    "            ad_pit_ensamble = qp.Ensemble(qp.hist, data=dict(bins=ad_pit_bins_edges, pdfs=np.array([ad_pit_dist])))\n",
    "            ad_pit_cdf = ad_pit_ensamble.cdf(ad_xvals)[0]\n",
    "            ad_uniform_ensamble = qp.Ensemble(qp.hist,\n",
    "                                              data=dict(bins=ad_pit_bins_edges, pdfs=np.array([ad_uniform_dist])))\n",
    "            ad_uniform_cdf = ad_uniform_ensamble.cdf(ad_xvals)[0]\n",
    "            numerator = ((ad_pit_cdf - ad_uniform_cdf)**2)\n",
    "            denominator = (ad_uniform_cdf*(1.-ad_uniform_cdf))\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                self._stat = np.sqrt(float(len(metrics._sample)) * np.trapz(np.nan_to_num(numerator/denominator), ad_uniform_cdf))\n",
    "            self._critical_values = None\n",
    "            self._significance_levels = None\n",
    "```\n",
    "\n",
    "For the Anderson-Darling test, the comparison to a uniform distribution is not available in scipy.stats.anderson method, so using it does not make sense. \n",
    "\n",
    "Let's remove the catastrophic autliers (as done in the paper), to see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = AD(metrics, ad_pit_min=0.01, ad_pit_max=0.99).stat\n",
    "ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: it does not help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/53909526/interpreting-the-anderson-darling-test-scipy \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point estimates metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> Since the main metrics still don't reproduce DC1 results, I have implemented the point estimate metrics (although it is out of the scope), as a second test, just to make sure there is nothing wrong with the initual steps of reading the samples and computing the PITs.  </font>\n",
    "\n",
    "<font color=red style=boldface> Questions for Sam/Alex: The point estimate statistics in table B1 refer to the total sample or after a magnitude cut? If the latter, what magnitude limit did you use? Do you think it can explain the tiny difference in the scatter?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample.plot_old_valid()#gals=gals, colors=colors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_metrics_table = sample.plot_old_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(old_metrics_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> TBD </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
